# -*- coding: utf-8 -*-
"""
dialect_mas_lora.py

Stage2:
  - Stage1 ë‹¨ì¼ LoRA ì–´ëŒ‘í„°ë¥¼ Whisper-smallì— merge â†’ ê³µí†µ ë² ì´ìŠ¤(ë‹¨ì¼ LoRAê¹Œì§€ í¬í•¨ëœ ëª¨ë¸)ë¡œ ì‚¬ìš©
  - ì´ ë² ì´ìŠ¤ëŠ” ì „ë¶€ freeze
  - Encoder self-attn q_proj, v_proj ì—ë§Œ MAS-LoRA(ë°©ì–¸ ì „ìš© expert 5ê°œ) ì£¼ì…

ë°ì´í„°:
  - <Day>, <News>, <Tel> ë“±ì€ ì „ë¶€ ì œê±°
  - <Dia><JL>, <Dia><GW>, <Dia><GS>, <Dia><JJ>, <Dia><CC> 5ê°œ ë°©ì–¸ë§Œ ì‚¬ìš©

í•™ìŠµ:
  - ê° ë°©ì–¸ ìƒ˜í”Œì€ ìê¸° expertë§Œ 1ì¸ one-hot gate (accent-aware)
  - OptimizerëŠ” MAS-LoRAì˜ A/B(As., Bs.) íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸
  - ê²€ì¦ì€ ë°©ì–¸ì— ëŒ€í•´ 5 expert ê· ë“±(1/5) mixtureë¡œ í‰ê°€ (ë‚˜ì¤‘ì— inference ëª¨ë“œë‘ ë§ì¶”ê¸° ì¢‹ê²Œ)
"""

import os, json, time, math, logging, random
from typing import Dict, Any, Optional, List

import numpy as np
import torch
import torch.nn as nn
from datasets import load_from_disk
from torch.utils.data import DataLoader
from transformers import WhisperProcessor, WhisperForConditionalGeneration, get_scheduler
from peft import PeftModel

import sys
sys.path.append("/home/work/cymg0001/preprocessed_audio/utils")
import kwhisper as kw

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê²½ë¡œ / ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DATA_DIR        = "/home/work/cymg0001/preprocessed_audio/sml_arrow_fixed_train"
OUTPUT_DIR      = "/home/work/cymg0001/preprocessed_audio/mg/dialect_mas_lora_output"
RUN_NAME        = "mas_lora_small_dialect_from_single"

BASE_MODEL_NAME = "openai/whisper-small"
LANGUAGE        = "ko"
TASK            = "transcribe"

# Stage1 ë‹¨ì¼ LoRA ì–´ëŒ‘í„° ê²½ë¡œ
SINGLE_LORA_DIR = "/home/work/cymg0001/preprocessed_audio/runs_lora/tel_off_lora_small/best"

# Processor: Stage1ì—ì„œ ì €ì¥í•œ processorê°€ ìˆìœ¼ë©´ ê·¸ê±° ì“°ê³ , ì—†ìœ¼ë©´ baseì—ì„œ ë¡œë”©
PROCESSOR_DIR   = os.path.join(SINGLE_LORA_DIR, "processor")

# í•™ìŠµ ì„¤ì •
EPOCHS             = 60
BATCH_SIZE         = 8
VAL_BATCH_SIZE     = 8
GRAD_ACCUM_STEPS   = 4
LEARNING_RATE      = 3e-5
WEIGHT_DECAY       = 0.0
WARMUP_FRAC        = 0.10
MAX_GRAD_NORM      = 1.0
VAL_RATIO          = 0.05
SEED               = 42

# Early Stopping
ES_PATIENCE        = 8
ES_DELTA           = 1e-6

# ë°©ì–¸ ë„ë©”ì¸ / expert ë§¤í•‘
ACTIVE_DIALECTS = [
    "<Dia><JL>",
    "<Dia><GW>",
    "<Dia><GS>",
    "<Dia><JJ>",
    "<Dia><CC>",
]
DOMAIN2IDX: Dict[str, int] = {d: i for i, d in enumerate(ACTIVE_DIALECTS)}
N_EXPERTS = len(ACTIVE_DIALECTS)

# MAS-LoRA í•˜ì´í¼ (encoder only)
MAS_R       = 8
MAS_ALPHA   = 16.0
MAS_DROPOUT = 0.1
TARGETS     = ("q_proj", "v_proj")

# ë¡œê¹…
SAVE_MODE        = True
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("mas-lora-dialect")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¬í˜„ì„± / AMP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cuda.matmul.allow_tf32 = True
    os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

def get_amp_dtype():
    if torch.cuda.is_available():
        return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    return torch.float32

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAS-LoRA ëª¨ë“ˆ (Stage1 ë‹¨ì¼ LoRA mergeëœ Linear ìœ„ì— expert residual)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MASLoRALinear(nn.Module):
    """
    - base: Stage1 ë‹¨ì¼ LoRAê¹Œì§€ mergeëœ nn.Linear (freeze)
    - As/Bs: ê° expert ë³„ LoRA íŒŒë¼ë¯¸í„° (trainable)
    - _expert_weights: (B, n_experts) â€” ì™¸ë¶€ì—ì„œ ì„¤ì •í•˜ëŠ” gate (one-hot / uniform ë“±)
    """
    def __init__(self, base: nn.Linear, r: int, alpha: float,
                 dropout: float, n_experts: int):
        super().__init__()
        assert isinstance(base, nn.Linear), "MASLoRALinear baseëŠ” nn.Linear ì—¬ì•¼ í•©ë‹ˆë‹¤."
        self.base = base
        self.n_experts = int(n_experts)
        self.r = int(r)
        self.scaling = alpha / max(1, self.r)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

        in_f  = base.in_features
        out_f = base.out_features
        self.in_features  = in_f
        self.out_features = out_f

        self.As = nn.ParameterList([
            nn.Parameter(torch.zeros(self.r, in_f))
            for _ in range(self.n_experts)
        ])
        self.Bs = nn.ParameterList([
            nn.Parameter(torch.zeros(out_f, self.r))
            for _ in range(self.n_experts)
        ])
        self.reset_parameters()

        self._expert_weights: Optional[torch.Tensor] = None

    def reset_parameters(self):
        for A, B in zip(self.As, self.Bs):
            nn.init.kaiming_uniform_(A, a=math.sqrt(5))
            nn.init.zeros_(B)

    @torch.no_grad()
    def set_expert_weights(self, w: Optional[torch.Tensor]):
        """
        w: (B, n_experts) or None
        """
        self._expert_weights = w

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # base path (Stage1 ë‹¨ì¼ LoRA mergeëœ Linear)
        out = self.base(x)

        if self.r == 0 or self._expert_weights is None:
            return out

        w = self._expert_weights
        dev = x.device
        if w.device != dev:
            w = w.to(dev)

        # expert íŒŒë¼ë¯¸í„°ë„ device ë™ê¸°í™”
        for i in range(self.n_experts):
            if self.As[i].device != dev:
                self.As[i].data = self.As[i].data.to(dev)
                self.Bs[i].data = self.Bs[i].data.to(dev)

        # x: (B, T, C) ë˜ëŠ” (B, C)
        if x.dim() == 3:
            Bsz, T, C = x.shape
            x2 = x.reshape(Bsz * T, C)
            # w: (B, E) â†’ (B*T, E)
            if w.dim() == 2 and w.shape[0] == Bsz:
                w2 = w.repeat_interleave(T, dim=0)
            else:
                w2 = w
        else:
            Bsz, C = x.shape
            x2 = x
            w2 = w  # (B, E)

        lora_sum = None
        for e in range(self.n_experts):
            A = self.As[e]      # (r, in_f)
            Bmat = self.Bs[e]   # (out_f, r)
            l = x2 @ A.t()      # (B*, r)
            l = self.dropout(l)
            l = l @ Bmat.t()    # (B*, out_f)
            we = w2[:, e].unsqueeze(1)  # (B*, 1)
            l = l * we
            lora_sum = l if lora_sum is None else (lora_sum + l)

        if x.dim() == 3:
            lora_sum = lora_sum.view(Bsz, T, self.out_features)

        return out + lora_sum * self.scaling


def inject_mas_encoder(model: nn.Module,
                       targets=("q_proj", "v_proj"),
                       r=8, alpha=16.0, dropout=0.1, n_experts=5):
    """
    Encoder self-attn q_proj/v_proj ìë¦¬ì— MASLoRALinear ì£¼ì….
    baseëŠ” í˜„ì¬ Linear (Stage1 ë¡œë¼ mergeëœ ìƒíƒœ)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    """
    replaced = 0
    for name, module in model.named_modules():
        # encoder self-attnë§Œ
        if ".encoder.layers." not in name:
            continue
        if ".self_attn." not in name:
            continue
        if not any(name.endswith(f".{t}") for t in targets):
            continue

        parent_name = name.rsplit(".", 1)[0]
        attr = name.split(".")[-1]
        parent = model.get_submodule(parent_name)
        sub = getattr(parent, attr, None)
        if not isinstance(sub, nn.Linear):
            continue

        mas = MASLoRALinear(sub, r=r, alpha=alpha, dropout=dropout, n_experts=n_experts)
        setattr(parent, attr, mas)
        replaced += 1

    logger.info(f"[MAS-ENC] injected {replaced} modules on encoder.self targets={targets}")


def set_all_mas_weights(model: nn.Module, w: Optional[torch.Tensor]):
    """
    ëª¨ë¸ ì•ˆì˜ ëª¨ë“  MASLoRALinear ëª¨ë“ˆì— ë™ì¼í•œ gate w(B, E)ë¥¼ ì„¤ì •
    """
    for m in model.modules():
        if isinstance(m, MASLoRALinear):
            m.set_expert_weights(w)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì €ì¥ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_model_and_proc(model, processor, outdir, tag, meta=None):
    path = os.path.join(outdir, tag)
    os.makedirs(path, exist_ok=True)
    try:
        model.save_pretrained(path)
    except Exception:
        pass
    try:
        processor.save_pretrained(os.path.join(path, "processor"))
    except Exception:
        pass
    if meta:
        with open(os.path.join(path, "meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
    logger.info(f"[SAVED] {tag} -> {path}")

    # MAS íŒŒë¼ë¯¸í„° í¬í•¨ state_dictë„ ì¶”ê°€ë¡œ ì €ì¥
    try:
        torch.save(model.state_dict(), os.path.join(path, "pytorch_mas.bin"))
    except Exception:
        logger.warning("state_dict ì €ì¥ ì‹¤íŒ¨ (ë¬´ì‹œí•´ë„ ë¨).")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„ë©”ì¸ í•„í„° / mas_idx ë¶€ì—¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _keep_dialect_only(ex: Dict[str, Any]) -> bool:
    """
    ë°©ì–¸ ë„ë©”ì¸ 5ê°œë§Œ ë‚¨ê¹€.
    <Day>, <News>, <Tel> ë“± ë°©ì–¸ì´ ì•„ë‹Œ ë„ë©”ì¸ì€ ëª¨ë‘ ì œê±°.
    """
    dom = ex.get("domain", None)
    return isinstance(dom, str) and dom in ACTIVE_DIALECTS

def _assign_mas_idx(ex: Dict[str, Any]) -> Dict[str, Any]:
    dom = ex.get("domain", None)
    ex["mas_idx"] = int(DOMAIN2IDX.get(dom, -1))
    return ex

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ í•™ìŠµ ë£¨í”„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    set_seed(SEED)
    kw.patch_whisper_forward_once()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    amp_dtype = get_amp_dtype()

    outdir = os.path.join(OUTPUT_DIR, RUN_NAME)
    os.makedirs(outdir, exist_ok=True)
    log_dir = os.path.join(outdir, "logs"); os.makedirs(log_dir, exist_ok=True)
    epoch_log_path = os.path.join(log_dir, "epoch_log.json")

    # Processor ë¡œë”© (Stage1ì—ì„œ ì €ì¥í•œ processor ìš°ì„ )
    if os.path.isdir(PROCESSOR_DIR):
        logger.info(f"[LOAD] processor from {PROCESSOR_DIR}")
        processor = WhisperProcessor.from_pretrained(PROCESSOR_DIR)
    else:
        logger.info(f"[LOAD] processor from base model {BASE_MODEL_NAME}")
        processor = WhisperProcessor.from_pretrained(BASE_MODEL_NAME, language=LANGUAGE, task=TASK)

    # Stage1 ë‹¨ì¼ LoRA merge â†’ ê³µí†µ ë² ì´ìŠ¤
    logger.info(f"[LOAD] base model={BASE_MODEL_NAME}")
    base_model = WhisperForConditionalGeneration.from_pretrained(BASE_MODEL_NAME)
    base_model.config.pad_token_id = processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id
    base_model.config.use_cache = False

    logger.info(f"[LOAD] Stage1 single LoRA from {SINGLE_LORA_DIR}")
    peft_model = PeftModel.from_pretrained(base_model, SINGLE_LORA_DIR)
    model = peft_model.merge_and_unload()
    model.to(device)

    # Encoderì— MAS-LoRA expert 5ê°œ ì£¼ì…
    inject_mas_encoder(model, TARGETS, r=MAS_R, alpha=MAS_ALPHA,
                       dropout=MAS_DROPOUT, n_experts=N_EXPERTS)

    # ëª¨ë“  íŒŒë¼ë¯¸í„° freeze í›„, MAS As/Bsë§Œ í•™ìŠµ
    for n, p in model.named_parameters():
        p.requires_grad = False
    mas_params = []
    for n, p in model.named_parameters():
        if "As." in n or "Bs." in n:
            p.requires_grad = True
            mas_params.append(p)
    logger.info(f"[Params] MAS trainable params = {sum(p.numel() for p in mas_params):,}")

    # ë°ì´í„° ë¡œë“œ
    logger.info(f"[Data] load_from_disk: {DATA_DIR}")
    ds_all = load_from_disk(DATA_DIR)

    # ë°©ì–¸ ë„ë©”ì¸ë§Œ ë‚¨ê¸°ê¸°
    n_total_before = ds_all.num_rows
    ds_all = ds_all.filter(_keep_dialect_only)
    n_total_after = ds_all.num_rows
    logger.info(f"[Filter] total: {n_total_before} -> {n_total_after} (dialect only)")
    logger.info(f"[Filter] ACTIVE_DIALECTS={ACTIVE_DIALECTS}")

    # mas_idx ë¶€ì—¬
    ds_all = ds_all.map(_assign_mas_idx)
    assert all(x != -1 for x in ds_all["mas_idx"]), "[ERROR] mas_idx=-1 (ë°©ì–¸ ë§¤í•‘ ì‹¤íŒ¨) ì¡´ì¬"

    # ë¼ë²¨/EOS ë³´ì¥
    ds_all = kw.ensure_labels_and_eos(ds_all, processor.tokenizer,
                                      text_key="text", labels_key="labels")

    # Train/Val split
    split = ds_all.train_test_split(test_size=VAL_RATIO, seed=SEED)
    train_ds, val_ds = split["train"], split["test"]
    logger.info(f"[Data] Train={len(train_ds)}  Val={len(val_ds)} (ratio={VAL_RATIO})")

    # labels_len ìƒì„± (ì›ë˜ MAS ì½”ë“œë‘ ë™ì¼ íŒ¨í„´)
    def _add_labels_len(batch):
        labs = batch["labels"]
        if hasattr(labs[0], "shape"):
            lens = [int(x.shape[0]) for x in labs]
        else:
            lens = [len(x) for x in labs]
        return {"labels_len": lens}

    train_ds = train_ds.map(_add_labels_len, batched=True)
    val_ds   = val_ds.map(_add_labels_len,   batched=True)

    # ë„ë©”ì¸ ë¶„í¬ í™•ì¸ (Train ê¸°ì¤€)
    train_mas_idx = np.array(train_ds["mas_idx"], dtype=np.int64)
    counts = np.bincount(train_mas_idx, minlength=N_EXPERTS)
    p_domain = counts / counts.sum()
    logger.info(f"[Domain] counts={counts.tolist()}  p_domain={p_domain.tolist()} (order={ACTIVE_DIALECTS})")

    # í¬ë§· ì„¤ì •
    cols = ["input_features", "attention_mask", "labels", "labels_len", "mas_idx"]
    train_ds = train_ds.with_format("torch", columns=cols, output_all_columns=True)
    val_ds   = val_ds.with_format("torch",   columns=cols, output_all_columns=True)

    # Collator (ê¸°ì¡´ collator + mas_idx ë¶™ì´ëŠ” ë˜í¼)
    base_collate = kw.make_data_collator(processor)

    def collate_with_mas(examples):
        # ê¸°ë³¸ collateë¡œ input_features, attention_mask, labels, labels_len ë“± ì²˜ë¦¬
        batch = base_collate(examples)
        # examplesì—ì„œ mas_idx ëª¨ì•„ì„œ í…ì„œë¡œ ì¶”ê°€
        mas_idx_list = []
        for ex in examples:
            mas_idx_list.append(int(ex["mas_idx"]))
        batch["mas_idx"] = torch.tensor(mas_idx_list, dtype=torch.long)
        return batch

    train_loader = DataLoader(
        train_ds,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=2,
        pin_memory=True,
        collate_fn=collate_with_mas,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=VAL_BATCH_SIZE,
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        collate_fn=collate_with_mas,
    )

    # Optim / Scheduler (MAS íŒŒë¼ë¯¸í„°ë§Œ)
    optimizer = torch.optim.AdamW(mas_params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    steps_per_epoch = math.ceil(len(train_loader) / max(1, GRAD_ACCUM_STEPS))
    total_steps = steps_per_epoch * EPOCHS
    warmup_steps = int(total_steps * WARMUP_FRAC)
    scheduler = get_scheduler("linear", optimizer=optimizer,
                              num_warmup_steps=warmup_steps, num_training_steps=total_steps)

    PAD_ID = processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id

    epoch_logs: List[Dict[str, Any]] = []
    best_val = float("inf"); best_epoch = -1; no_improve = 0
    global_step = 0

    logger.info(f"â–¶ Train start(MAS dialect) â€” epochs={EPOCHS}, experts={N_EXPERTS}")

    for epoch in range(1, EPOCHS + 1):
        t0 = time.time()
        model.train()
        tr_loss_sum = 0.0
        optimizer.zero_grad(set_to_none=True)

        # ===== Train =====
        for step, batch in enumerate(train_loader, start=1):
            mas_idx = batch.get("mas_idx", None)
            labels_len = batch.get("labels_len", None)

            if mas_idx is None:
                raise KeyError("batch['mas_idx']ê°€ collatorì—ì„œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                               "collate_with_mas êµ¬í˜„ì„ í™•ì¸í•˜ì„¸ìš”.")
            if labels_len is None:
                raise KeyError("batch['labels_len']ê°€ ì—†ìŠµë‹ˆë‹¤. _add_labels_len / with_format êµ¬í˜„ í™•ì¸ í•„ìš”.")

            # Tensorë§Œ deviceë¡œ
            batch_t = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                       for k, v in batch.items()}

            mas_idx_t = batch_t["mas_idx"]
            labels_len_t = batch_t["labels_len"]

            # one-hot gate (ê° ë°©ì–¸ì€ ìê¸° expertë§Œ ON)
            Bsz = mas_idx_t.shape[0]
            W = torch.zeros(Bsz, N_EXPERTS, device=device)
            W[torch.arange(Bsz, device=device), mas_idx_t] = 1.0
            set_all_mas_weights(model, W)

            # labels padâ†’-100
            labels = kw.labels_pad_to_ignore_by_len(batch_t["labels"], labels_len_t)

            with torch.autocast(device_type="cuda" if device == "cuda" else "cpu", dtype=amp_dtype):
                out = model(
                    input_features=batch_t["input_features"],
                    attention_mask=batch_t.get("attention_mask", None),
                    labels=labels,
                )
                loss = out.loss / GRAD_ACCUM_STEPS

            loss.backward()
            tr_loss_sum += loss.item()

            if step % GRAD_ACCUM_STEPS == 0:
                if MAX_GRAD_NORM:
                    nn.utils.clip_grad_norm_(mas_params, MAX_GRAD_NORM)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad(set_to_none=True)
                global_step += 1

        train_loss_epoch = (tr_loss_sum / max(1, steps_per_epoch)) * GRAD_ACCUM_STEPS

        # ===== Validation =====
        model.eval()
        val_loss_sum = 0.0
        with torch.no_grad():
            for batch in val_loader:
                labels_len = batch.get("labels_len", None)
                if labels_len is None:
                    raise KeyError("val batch ì— labels_len ëˆ„ë½ â€” trainê³¼ ë™ì¼í•˜ê²Œ _add_labels_len/with_format í•„ìš”.")

                batch_t = {k: (v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v)
                           for k, v in batch.items()}
                labels_len_t = batch_t["labels_len"]

                # â˜… ê²€ì¦ ì‹œì—ëŠ” ë°©ì–¸ì— ëŒ€í•´ 5 expert ê· ë“±(1/5) mixtureë¡œ í‰ê°€
                Bsz = batch_t["labels"].shape[0]
                W = torch.full((Bsz, N_EXPERTS), 1.0 / N_EXPERTS, device=device)
                set_all_mas_weights(model, W)

                labels = kw.labels_pad_to_ignore_by_len(batch_t["labels"], labels_len_t)

                with torch.autocast(device_type="cuda" if device == "cuda" else "cpu", dtype=amp_dtype):
                    out = model(
                        input_features=batch_t["input_features"],
                        attention_mask=batch_t.get("attention_mask", None),
                        labels=labels,
                    )
                    val_loss_sum += out.loss.item()

        val_loss_epoch = val_loss_sum / max(1, len(val_loader))
        dt = time.time() - t0

        improved = (best_val - val_loss_epoch) > ES_DELTA
        if improved:
            best_val, best_epoch = val_loss_epoch, epoch
            no_improve = 0
            status = "â†‘ best"
            if SAVE_MODE:
                save_model_and_proc(
                    model, processor, outdir, "best",
                    meta={
                        "epoch": epoch,
                        "val_loss": float(val_loss_epoch),
                        "train_loss": float(train_loss_epoch),
                        "p_domain": p_domain.tolist(),
                        "domains": ACTIVE_DIALECTS,
                    }
                )
        else:
            no_improve += 1
            status = f"â†’ no improve ({no_improve}/{ES_PATIENCE})"

        logger.info("[Epoch %03d] train_loss=%.4f | val_loss=%.4f | %s | %.1fs",
                    epoch, train_loss_epoch, val_loss_epoch, status, dt)

        epoch_logs.append({
            "epoch": epoch,
            "train_loss": float(train_loss_epoch),
            "val_loss": float(val_loss_epoch),
            "global_step": int(global_step),
            "no_improve": int(no_improve),
            "time_sec": float(dt),
        })
        with open(epoch_log_path, "w", encoding="utf-8") as f:
            json.dump(epoch_logs, f, ensure_ascii=False, indent=2)

        if ES_PATIENCE and no_improve >= ES_PATIENCE:
            logger.info("[EARLY STOP] best@%d (val=%.4f)", best_epoch, best_val)
            break

        if device == "cuda":
            torch.cuda.empty_cache()

    # final ì €ì¥
    if SAVE_MODE:
        meta = {
            "epoch": epoch,
            "val_loss": float(val_loss_epoch),
            "best_epoch": best_epoch,
            "best_val": float(best_val),
        }
        save_model_and_proc(model, processor, outdir, "final", meta)

    logger.info("âœ… ì™„ë£Œ. Best@%d val=%.4f", best_epoch, best_val)
    logger.info("ğŸ—‚ Logs: %s", epoch_log_path)


if __name__ == "__main__":
    main()
